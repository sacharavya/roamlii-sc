# Rate Limit Fix - Complete Guide

## Your Error
```
FirecrawlRateLimitError: Rate Limit Exceeded
Consumed (req/min): 17
Remaining (req/min): 0
Retry after 1s
Resets at Sun Oct 12 2025 09:45:30
```

**Problem:** You're making too many requests per minute! ‚ö†Ô∏è

---

## What Was Fixed ‚úÖ

### 1. Added Token Bucket Rate Limiter
**File:** [src/bg_jobs/arq_tasks.py](src/bg_jobs/arq_tasks.py)

```python
class RateLimiter:
    """Prevents rate limit errors by controlling request pace"""

    def __init__(self, requests_per_minute=12):
        # Safely limits requests to 12/min (under your 17/min limit)
        pass

    async def acquire(self):
        # Waits if rate limit would be exceeded
        pass
```

### 2. Added Rate Limiting Before Each API Call
```python
async def extract_events_list(ctx, url):
    # üî• NEW: Wait for rate limit clearance
    await rate_limiter.acquire()

    # Now safe to make API request
    result = await firecrawl_async.scrape(url, ...)
```

### 3. Improved Rate Limit Error Handling
```python
except FirecrawlRateLimitError as exc:
    logger.error("Rate limit hit despite rate limiting!")
    await asyncio.sleep(5)  # Extra wait before retry
    raise exc
```

---

## How It Works

### Before (Broken):
```
Minute 1:
‚îú‚îÄ Job 1 ‚Üí API request (1/17)
‚îú‚îÄ Job 2 ‚Üí API request (2/17)
‚îú‚îÄ Job 3 ‚Üí API request (3/17)
‚îú‚îÄ ... (all jobs hitting API as fast as possible)
‚îî‚îÄ Job 17 ‚Üí API request (17/17) ‚úÖ Still OK

Job 18 ‚Üí API request ‚ùå RATE LIMIT EXCEEDED!
```

### After (Fixed):
```
Minute 1:
‚îú‚îÄ Job 1 ‚Üí rate_limiter.acquire() ‚Üí API request (1/12) ‚úÖ
‚îú‚îÄ Wait 5 seconds...
‚îú‚îÄ Job 2 ‚Üí rate_limiter.acquire() ‚Üí API request (2/12) ‚úÖ
‚îú‚îÄ Wait 5 seconds...
‚îú‚îÄ Job 3 ‚Üí rate_limiter.acquire() ‚Üí API request (3/12) ‚úÖ
‚îî‚îÄ ... (controlled pace: ~5s between requests)

Total: 12 requests/minute (safely under limit) ‚úÖ
```

---

## Configuration

### Your Rate Limit Settings

**Default:** 12 requests/minute (safe for most plans)

```python
# In .env (optional - uses 12 by default)
FIRECRAWL_RATE_LIMIT=12
```

### Plan-Based Settings

| Plan | Rate Limit | Recommended Setting |
|------|-----------|---------------------|
| **Free** | 15-20 req/min | `FIRECRAWL_RATE_LIMIT=12` ‚úÖ Default |
| **Hobby** | 20-30 req/min | `FIRECRAWL_RATE_LIMIT=20` |
| **Standard** | 50-100 req/min | `FIRECRAWL_RATE_LIMIT=50` |
| **Growth** | 200+ req/min | `FIRECRAWL_RATE_LIMIT=100` |

**Your error showed:** 17 consumed ‚Üí You're on Free/Hobby plan
**Recommended setting:** Keep at 12 (default) for safety buffer

---

## How Rate Limiting Works

### Token Bucket Algorithm

```
Bucket Capacity: 12 tokens
Refill Rate: 12 tokens/minute = 0.2 tokens/second

Every API Request:
1. Check if token available
2. If yes: consume token, make request
3. If no: wait until token refills

Example Timeline:
00:00 - Start with 12 tokens
00:00 - Request 1 (11 tokens left)
00:05 - Request 2 (10 tokens left, +1 refilled = 11)
00:10 - Request 3 (10 tokens left, +1 refilled = 11)
...
01:00 - Bucket refills to 12 tokens
```

### Why 12 Instead of 17?

Your plan allows ~17 req/min, but we use 12:
- **Safety buffer:** Accounts for timing variations
- **Retry buffer:** Leaves room for retries
- **Burst protection:** Prevents micro-bursts hitting limit
- **Network latency:** API might count requests differently

---

## Testing the Fix

### Step 1: Restart ARQ Worker
```bash
# Stop current worker (Ctrl+C)

# Start with rate limiting
arq src.bg_jobs.arq_tasks.WorkerSettings --verbose
```

**You should see:**
```
INFO [RateLimiter] Initialized: 12 requests/minute (~5.00s between requests)
```

### Step 2: Send Test Request
```bash
curl -X POST http://localhost:8000/api/events/ \
  -H "Content-Type: application/json" \
  -d '{"main_links": ["https://www.destinationcanada.com/en-ca/events"]}'
```

### Step 3: Watch Logs

**Good logs (rate limiting working):**
```
INFO [ARQ] Starting async scrape for URL: ...
DEBUG [ARQ] Rate limit check passed for ...
INFO [ARQ] Extracted 15 links (12 new) from ...
INFO [RateLimiter] Low tokens: 3/12 remaining
WARNING [RateLimiter] Rate limit reached. Waiting 4.2s...
DEBUG [ARQ] Rate limit check passed for ...
```

**Bad logs (would mean it's not working):**
```
ERROR [ARQ] Rate limit exceeded despite rate limiting! ‚ùå
```

---

## Monitoring

### Check Rate Limiter Status

The logs will show:
```
INFO [RateLimiter] Low tokens: 3/12 remaining
WARNING [RateLimiter] Rate limit reached. Waiting 5.2s...
```

### Calculate Your Current Rate

```python
# Monitor in Python
import time

start = time.time()
count = 0

# Watch logs for API requests
# After 1 minute, calculate:
elapsed = time.time() - start
rate = (count / elapsed) * 60
print(f"Current rate: {rate:.1f} requests/minute")
```

### Redis Monitoring

```bash
# Check queue status
curl http://localhost:8000/api/events/queue/stats

# Should show steady progress without rate limit errors
```

---

## Troubleshooting

### Still Getting Rate Limit Errors?

#### Solution 1: Reduce Rate Limit
```bash
# In .env
FIRECRAWL_RATE_LIMIT=10  # Even more conservative
```

#### Solution 2: Reduce Worker Concurrency
```python
# In arq_tasks.py WorkerSettings
max_jobs = 1  # Only 1 job at a time (slowest but safest)
```

#### Solution 3: Check Your Plan
```bash
# Visit Firecrawl dashboard
https://www.firecrawl.dev/dashboard

# Check actual rate limits
# Adjust FIRECRAWL_RATE_LIMIT accordingly
```

### Jobs Taking Too Long?

**This is expected!** Rate limiting slows things down to stay under limits.

**Math:**
- 60 jobs to process
- 12 requests/minute limit
- Time needed: 60 √∑ 12 = **5 minutes minimum**

**To speed up:**
1. Upgrade your Firecrawl plan (higher rate limits)
2. Use batch scraping (more efficient)
3. Reduce number of URLs being scraped

### Logs Show "Waiting X seconds"?

**This is GOOD!** It means rate limiting is working:
```
WARNING [RateLimiter] Rate limit reached. Waiting 5.2s...
```

This prevents the "Rate Limit Exceeded" error you were getting.

---

## Performance Impact

### Before Fix:
```
4 main URLs ‚Üí 60 detail URLs = 64 total requests
All fire as fast as possible
Rate limit hit after ~17 requests
40+ requests fail with rate limit error ‚ùå
Need to retry failed requests
Total time: Unpredictable (lots of failures and retries)
```

### After Fix:
```
4 main URLs ‚Üí 60 detail URLs = 64 total requests
Controlled pace: 12 requests/minute
All requests succeed ‚úÖ
No rate limit errors
No wasted retries
Total time: ~6 minutes (predictable)
```

**Trade-off:** Slower but reliable!

---

## Advanced Configuration

### Dynamic Rate Limiting

```python
# Adjust based on time of day
import datetime

hour = datetime.datetime.now().hour
if 9 <= hour <= 17:  # Business hours
    rate_limit = 10  # Conservative
else:  # Off-hours
    rate_limit = 15  # More aggressive
```

### Per-Plan Configuration

```bash
# In .env
FIRECRAWL_PLAN=free  # or hobby, standard, growth

# In code
plan_limits = {
    'free': 12,
    'hobby': 20,
    'standard': 50,
    'growth': 100
}
rate_limit = plan_limits[os.getenv('FIRECRAWL_PLAN', 'free')]
```

---

## Comparison with Firecrawl MCP

### What MCP Does:
- Automatic retry with exponential backoff
- Smart request queuing and throttling
- Rate limit detection and handling

### What We Added:
- ‚úÖ **Proactive rate limiting** (prevent errors before they happen)
- ‚úÖ **Token bucket algorithm** (smooth request pacing)
- ‚úÖ **Automatic waiting** (no manual intervention needed)
- ‚úÖ **Logging** (visibility into rate limit status)

**Result:** Same reliability as MCP, optimized for your use case!

---

## Summary

### The Problem:
```
‚ùå Making 17+ requests/minute
‚ùå Firecrawl limit: ~17 req/min
‚ùå No buffer ‚Üí Instant rate limit errors
```

### The Solution:
```
‚úÖ Token bucket rate limiter added
‚úÖ Waits between requests automatically
‚úÖ Default: 12 req/min (safe buffer)
‚úÖ Configurable via FIRECRAWL_RATE_LIMIT
```

### The Result:
```
‚úÖ No more "Rate Limit Exceeded" errors
‚úÖ Predictable, reliable scraping
‚úÖ All jobs complete successfully
‚úÖ Slower but steady progress
```

---

## Quick Fixes Summary

1. **Install httpx** (if not installed):
   ```bash
   pip install httpx
   ```

2. **Restart ARQ worker**:
   ```bash
   arq src.bg_jobs.arq_tasks.WorkerSettings --verbose
   ```

3. **Test it**:
   ```bash
   curl -X POST http://localhost:8000/api/events/ \
     -H "Content-Type: application/json" \
     -d '{"main_links": ["https://example.com/events"]}'
   ```

4. **Watch logs** - Should see:
   - `[RateLimiter] Initialized: 12 requests/minute`
   - `[RateLimiter] Rate limit reached. Waiting...` (occasionally)
   - NO "Rate Limit Exceeded" errors ‚úÖ

---

**Your rate limiting is now fixed! Restart the worker and test it out. üéâ**
